{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"training/output_bert\")\n",
    "tokenizer = BertTokenizer.from_pretrained('training/output_bert')\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"new_quantization/original_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration: model and dataset\n",
    "model_name = \"bert-base-uncased\"\n",
    "dataset_name = \"glue\"\n",
    "dataset_config = \"sst2\"\n",
    "split = \"validation\"  # SST-2 test split is unlabeled, so we use validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom model class that replicates the quantized structure\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bit_precision=8):\n",
    "        super(QuantizedLinear, self).__init__()\n",
    "        self.base_layer = nn.Linear(in_features, out_features)\n",
    "        self.bit_precision = bit_precision\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        return self.base_layer(input_tensor)\n",
    "        \n",
    "    def quantize(self, tensor, bits):\n",
    "        # Quantization logic (not needed for loading)\n",
    "        return tensor\n",
    "\n",
    "# Function to create a model with the same structure as the quantized model\n",
    "def create_quantized_model_structure(config, layer_precision_map=None):\n",
    "    # Start with a standard model\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "    \n",
    "    # Replace linear layers with QuantizedLinear layers\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            parent_name = name.rsplit('.', 1)[0] if '.' in name else ''\n",
    "            layer_name = name.rsplit('.', 1)[1] if '.' in name else name\n",
    "            \n",
    "            # Extract layer number if it exists\n",
    "            layer_num = None\n",
    "            if 'layer.' in parent_name:\n",
    "                try:\n",
    "                    layer_num = int(parent_name.split('layer.')[1].split('.')[0])\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Get precision for this layer\n",
    "            bits = 8  # Default\n",
    "            if layer_precision_map and layer_num is not None:\n",
    "                bits = layer_precision_map.get(layer_num, 8)\n",
    "            \n",
    "            # Create quantized layer\n",
    "            in_features = module.in_features\n",
    "            out_features = module.out_features\n",
    "            \n",
    "            # Get parent module\n",
    "            parent = model\n",
    "            if parent_name:\n",
    "                for part in parent_name.split('.'):\n",
    "                    parent = getattr(parent, part)\n",
    "            \n",
    "            # Replace the layer\n",
    "            quant_layer = QuantizedLinear(in_features, out_features, bits)\n",
    "            setattr(parent, layer_name, quant_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_quantized_model(model_dir=\"pi_bert_new_method\"):\n",
    "    \"\"\"\n",
    "    Load a quantized model from the given directory.\n",
    "    \"\"\"\n",
    "    # Load model configuration\n",
    "    config_path = os.path.join(model_dir, \"model_config\", \"config.json\")\n",
    "    config = AutoConfig.from_pretrained(config_path)\n",
    "    \n",
    "    # Load layer precision map if available\n",
    "    layer_precision_map = {}\n",
    "    precision_map_path = os.path.join(model_dir, \"layer_precision_map.json\")\n",
    "    if os.path.exists(precision_map_path):\n",
    "        with open(precision_map_path, \"r\") as f:\n",
    "            layer_precision_map = json.load(f)\n",
    "            # Convert string keys back to integers\n",
    "            layer_precision_map = {int(k): v for k, v in layer_precision_map.items()}\n",
    "    \n",
    "    # Create model with quantized structure\n",
    "    model = create_quantized_model_structure(config, layer_precision_map)\n",
    "    \n",
    "    # Load the saved state dict\n",
    "    model_path = os.path.join(model_dir, \"quantized_model.pth\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, tokenizer, layer_precision_map\n",
    "\n",
    "# Test loading the model\n",
    "try:\n",
    "    loaded_model, loaded_tokenizer, loaded_precision_map = load_quantized_model()\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"Layer precision map: {loaded_precision_map}\")\n",
    "    \n",
    "    # Test a prediction with the loaded model\n",
    "    def predict_with_loaded_model(text, model=loaded_model, tokenizer=loaded_tokenizer):\n",
    "        # Get the device the model is on\n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pred_idx = torch.argmax(scores, dim=-1).item()\n",
    "        confidence = scores[0][pred_idx].item()\n",
    "        label = \"Positive\" if pred_idx == 1 else \"Negative\"\n",
    "        \n",
    "        return label, confidence\n",
    "    \n",
    "    # Test with the same example as before\n",
    "    test_text = \"This movie was fantastic! I really enjoyed it.\"\n",
    "    label, confidence = predict_with_loaded_model(test_text)\n",
    "    print(f\"Prediction: {label} (confidence: {confidence:.4f})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standalone script for loading the quantized model\n",
    "standalone_script = '''\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom module that wraps a linear layer for quantization.\n",
    "    This structure matches how the model was saved.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bit_precision=8):\n",
    "        super(QuantizedLinear, self).__init__()\n",
    "        self.base_layer = nn.Linear(in_features, out_features)\n",
    "        self.bit_precision = bit_precision\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        return self.base_layer(input_tensor)\n",
    "        \n",
    "    def quantize(self, tensor, bits):\n",
    "        # Quantization logic (not needed for loading)\n",
    "        return tensor\n",
    "\n",
    "def create_quantized_model_structure(config, layer_precision_map=None):\n",
    "    \"\"\"\n",
    "    Create a model with the same structure as the quantized model.\n",
    "    \n",
    "    Args:\n",
    "        config: Model configuration\n",
    "        layer_precision_map: Dictionary mapping layer numbers to bit precision\n",
    "        \n",
    "    Returns:\n",
    "        Model with quantized structure\n",
    "    \"\"\"\n",
    "    # Start with a standard model\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "    \n",
    "    # Replace linear layers with QuantizedLinear layers\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            parent_name = name.rsplit('.', 1)[0] if '.' in name else ''\n",
    "            layer_name = name.rsplit('.', 1)[1] if '.' in name else name\n",
    "            \n",
    "            # Extract layer number if it exists\n",
    "            layer_num = None\n",
    "            if 'layer.' in parent_name:\n",
    "                try:\n",
    "                    layer_num = int(parent_name.split('layer.')[1].split('.')[0])\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Get precision for this layer\n",
    "            bits = 8  # Default\n",
    "            if layer_precision_map and layer_num is not None:\n",
    "                bits = layer_precision_map.get(layer_num, 8)\n",
    "            \n",
    "            # Create quantized layer\n",
    "            in_features = module.in_features\n",
    "            out_features = module.out_features\n",
    "            \n",
    "            # Get parent module\n",
    "            parent = model\n",
    "            if parent_name:\n",
    "                for part in parent_name.split('.'):\n",
    "                    parent = getattr(parent, part)\n",
    "            \n",
    "            # Replace the layer\n",
    "            quant_layer = QuantizedLinear(in_features, out_features, bits)\n",
    "            setattr(parent, layer_name, quant_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_quantized_model(model_dir=\"pi_bert\"):\n",
    "    \"\"\"\n",
    "    Load a quantized model from the given directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing the saved model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer, layer_precision_map)\n",
    "    \"\"\"\n",
    "    # Load model configuration\n",
    "    config_path = os.path.join(model_dir, \"model_config\", \"config.json\")\n",
    "    config = AutoConfig.from_pretrained(config_path)\n",
    "    \n",
    "    # Load layer precision map if available\n",
    "    layer_precision_map = {}\n",
    "    precision_map_path = os.path.join(model_dir, \"layer_precision_map.json\")\n",
    "    if os.path.exists(precision_map_path):\n",
    "        with open(precision_map_path, \"r\") as f:\n",
    "            layer_precision_map = json.load(f)\n",
    "            # Convert string keys back to integers\n",
    "            layer_precision_map = {int(k): v for k, v in layer_precision_map.items()}\n",
    "    \n",
    "    # Create model with quantized structure\n",
    "    model = create_quantized_model_structure(config, layer_precision_map)\n",
    "    \n",
    "    # Load the saved state dict\n",
    "    model_path = os.path.join(model_dir, \"quantized_model.pth\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, tokenizer, layer_precision_map\n",
    "\n",
    "def predict(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Make a sentiment prediction using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to classify\n",
    "        model: The loaded model\n",
    "        tokenizer: The loaded tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (label, confidence)\n",
    "    \"\"\"\n",
    "    # Get the device the model is on\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_idx = torch.argmax(scores, dim=-1).item()\n",
    "    confidence = scores[0][pred_idx].item()\n",
    "    label = \"Positive\" if pred_idx == 1 else \"Negative\"\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    try:\n",
    "        model, tokenizer, precision_map = load_quantized_model()\n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        # Test with some examples\n",
    "        examples = [\n",
    "            \"This movie was fantastic! I really enjoyed it.\",\n",
    "            \"The acting was terrible and the plot made no sense.\",\n",
    "            \"It was okay, but I wouldn't watch it again.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\\\nMaking predictions with the quantized model:\")\n",
    "        for text in examples:\n",
    "            label, confidence = predict(text, model, tokenizer)\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Prediction: {label} (confidence: {confidence:.4f})\")\n",
    "            print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "'''\n",
    "\n",
    "# Write the standalone script to a file\n",
    "with open(\"load_quantized_model.py\", \"w\") as f:\n",
    "    f.write(standalone_script)\n",
    "\n",
    "print(\"Standalone script created: load_quantized_model.py\")\n",
    "print(\"You can now use this script to load and use the quantized model from scratch.\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"```python\")\n",
    "print(\"from load_quantized_model import load_quantized_model, predict\")\n",
    "print(\"\")\n",
    "print(\"# Load the model\")\n",
    "print(\"model, tokenizer, _ = load_quantized_model()\")\n",
    "print(\"\")\n",
    "print(\"# Make a prediction\")\n",
    "print(\"text = \\\"This movie was fantastic!\\\"\")\n",
    "print(\"label, confidence = predict(text, model, tokenizer)\")\n",
    "print(\"print(f\\\"Prediction: {label} (confidence: {confidence:.4f})\\\")\")\n",
    "print(\"```\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['sentence'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a proper standalone script for loading the quantized model\n",
    "proper_script = '''\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "def load_quantized_model(model_dir=\"pi_bert_new_method\"):\n",
    "    \"\"\"\n",
    "    Load a quantized model from the given directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing the saved model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer, layer_precision_map)\n",
    "    \"\"\"\n",
    "    # Load model configuration\n",
    "    config_path = os.path.join(model_dir, \"model_config\", \"config.json\")\n",
    "    config = AutoConfig.from_pretrained(config_path)\n",
    "    \n",
    "    # Create a model with the same architecture as the original\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"training/output_bert\")\n",
    "    \n",
    "    # Load the quantized state dict\n",
    "    model_path = os.path.join(model_dir, \"quantized_model.pth\")\n",
    "    state_dict = torch.load(model_path)\n",
    "    \n",
    "    # Process the state dict to match the model structure\n",
    "    # This handles the case where the saved model has different layer structure\n",
    "    # (e.g., base_layer components) than the loaded model\n",
    "    new_state_dict = OrderedDict()\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        if \".base_layer.\" in key:\n",
    "            # Remove the base_layer part from keys\n",
    "            new_key = key.replace(\".base_layer\", \"\")\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "    \n",
    "    # Load the processed state dict into the model\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    # Load layer precision map if available\n",
    "    layer_precision_map = {}\n",
    "    precision_map_path = os.path.join(model_dir, \"layer_precision_map.json\")\n",
    "    if os.path.exists(precision_map_path):\n",
    "        with open(precision_map_path, \"r\") as f:\n",
    "            layer_precision_map = json.load(f)\n",
    "            # Convert string keys back to integers\n",
    "            layer_precision_map = {int(k): v for k, v in layer_precision_map.items()}\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, tokenizer, layer_precision_map\n",
    "\n",
    "def predict(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Make a sentiment prediction using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to classify\n",
    "        model: The loaded model\n",
    "        tokenizer: The loaded tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (label, confidence)\n",
    "    \"\"\"\n",
    "    # Get the device the model is on\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_idx = torch.argmax(scores, dim=-1).item()\n",
    "    confidence = scores[0][pred_idx].item()\n",
    "    label = \"Positive\" if pred_idx == 1 else \"Negative\"\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    try:\n",
    "        model, tokenizer, precision_map = load_quantized_model()\n",
    "        print(\"Model loaded successfully!\")\n",
    "        print(f\"Layer precision map: {precision_map}\")\n",
    "        \n",
    "        # Test with some examples\n",
    "        examples = [\n",
    "            \"This movie was fantastic! I really enjoyed it.\",\n",
    "            \"The acting was terrible and the plot made no sense.\",\n",
    "            \"It was okay, but I wouldn\\'t watch it again.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\\\nMaking predictions with the quantized model:\")\n",
    "        for text in examples:\n",
    "            label, confidence = predict(text, model, tokenizer)\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Prediction: {label} (confidence: {confidence:.4f})\")\n",
    "            print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "'''\n",
    "\n",
    "# Write the proper standalone script to a file\n",
    "with open(\"load_quantized_model_proper.py\", \"w\") as f:\n",
    "    f.write(proper_script)\n",
    "\n",
    "print(\"Proper standalone script created: load_quantized_model_proper.py\")\n",
    "print(\"This script properly loads the quantized weights and handles the base_layer structure.\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"```python\")\n",
    "print(\"from load_quantized_model_proper import load_quantized_model, predict\")\n",
    "print(\"\")\n",
    "print(\"# Load the model\")\n",
    "print(\"model, tokenizer, precision_map = load_quantized_model()\")\n",
    "print(\"\")\n",
    "print(\"# Make a prediction\")\n",
    "print(\"text = \\\"This movie was fantastic!\\\"\")\n",
    "print(\"label, confidence = predict(text, model, tokenizer)\")\n",
    "print(\"print(f\\\"Prediction: {label} (confidence: {confidence:.4f})\\\")\")\n",
    "print(\"```\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the proper loading script\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "def load_quantized_model_test(model_dir=\"pi_bert_new_method\"):\n",
    "    \"\"\"\n",
    "    Load a quantized model from the given directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing the saved model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer, layer_precision_map)\n",
    "    \"\"\"\n",
    "    # Load model configuration\n",
    "    config_path = os.path.join(model_dir, \"model_config\", \"config.json\")\n",
    "    config = AutoConfig.from_pretrained(config_path)\n",
    "    \n",
    "    # Create a model with the same architecture as the original\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"training/output_bert\")\n",
    "    \n",
    "    # Load the quantized state dict\n",
    "    model_path = os.path.join(model_dir, \"quantized_model.pth\")\n",
    "    state_dict = torch.load(model_path)\n",
    "    \n",
    "    # Process the state dict to match the model structure\n",
    "    # This handles the case where the saved model has different layer structure\n",
    "    # (e.g., base_layer components) than the loaded model\n",
    "    new_state_dict = OrderedDict()\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        if \".base_layer.\" in key:\n",
    "            # Remove the base_layer part from keys\n",
    "            new_key = key.replace(\".base_layer\", \"\")\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "    \n",
    "    # Load the processed state dict into the model\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    # Load layer precision map if available\n",
    "    layer_precision_map = {}\n",
    "    precision_map_path = os.path.join(model_dir, \"layer_precision_map.json\")\n",
    "    if os.path.exists(precision_map_path):\n",
    "        with open(precision_map_path, \"r\") as f:\n",
    "            layer_precision_map = json.load(f)\n",
    "            # Convert string keys back to integers\n",
    "            layer_precision_map = {int(k): v for k, v in layer_precision_map.items()}\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, tokenizer, layer_precision_map\n",
    "\n",
    "def predict_test(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Make a sentiment prediction using the loaded model.\n",
    "    \"\"\"\n",
    "    # Get the device the model is on\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_idx = torch.argmax(scores, dim=-1).item()\n",
    "    confidence = scores[0][pred_idx].item()\n",
    "    label = \"Positive\" if pred_idx == 1 else \"Negative\"\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "# Test the loading function\n",
    "try:\n",
    "    print(\"Testing the proper loading function...\")\n",
    "    model, tokenizer, precision_map = load_quantized_model_test()\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"Layer precision map: {precision_map}\")\n",
    "    \n",
    "    # Test with an example\n",
    "    test_text = \"This movie was fantastic! I really enjoyed it.\"\n",
    "    label, confidence = predict_test(test_text, model, tokenizer)\n",
    "    print(f\"Prediction: {label} (confidence: {confidence:.4f})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load and tokenize the SST-2 dataset\n",
    "dataset = load_dataset(dataset_name, dataset_config, split=f\"{split}\")  # first 10% for speed\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# 6. DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to introduce sparsity to a given layer\n",
    "def introduce_layer_sparsity(layer, sparsity_ratio):\n",
    "    \"\"\"Apply sparsity to layer parameters by zeroing out values below threshold.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in layer.parameters():\n",
    "            flat_param = param.data.view(-1)\n",
    "            threshold = torch.quantile(torch.abs(flat_param), sparsity_ratio)\n",
    "            mask = torch.abs(flat_param) > threshold\n",
    "            param.data *= mask.float().view(param.data.shape)\n",
    "\n",
    "# Function to evaluate the model's accuracy\n",
    "def calculate_model_accuracy(model):\n",
    "    \"\"\"Evaluate model accuracy on the dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "    return correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 73.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Accuracy: 0.9277522935779816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layer_sensitivity = {}\n",
    "base_accuracy = calculate_model_accuracy(model)\n",
    "print(f\"Base Accuracy: {base_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 872/872 [00:12<00:00, 71.06it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 78.88it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:12<00:00, 71.85it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 76.81it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:10<00:00, 80.99it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 78.89it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 79.04it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 77.36it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:10<00:00, 81.27it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 78.85it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 73.16it/s]\n",
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 76.96it/s]\n"
     ]
    }
   ],
   "source": [
    "sparsity_ratio = 0.3\n",
    "sensitivities_values = []\n",
    "for layer_num, layer in enumerate(model.bert.encoder.layer):\n",
    "    layer_name = f\"Encoder_Layer {layer_num}\"\n",
    "    exported_model = AutoModelForSequenceClassification.from_pretrained(\"training/output_bert\").to(device)\n",
    "    exported_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    introduce_layer_sparsity(exported_model.bert.encoder.layer[layer_num], sparsity_ratio)\n",
    "    accuracy = calculate_model_accuracy(exported_model)\n",
    "    sensitivity_value = base_accuracy - accuracy\n",
    "    sensitivities_values.append(sensitivity_value)\n",
    "    del exported_model\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    else:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_0': 0.022935779816513735, 'layer_1': 0.011467889908256867, 'layer_2': 0.013761467889908174, 'layer_3': 0.04816513761467889, 'layer_4': 0.04013761467889909, 'layer_5': 0.016055045871559592, 'layer_6': 0.010321100917431103, 'layer_7': 0.002293577981651307, 'layer_8': 0.008027522935779796, 'layer_9': 0.002293577981651307, 'layer_10': 0.0, 'layer_11': 0.0011467889908256534}\n"
     ]
    }
   ],
   "source": [
    "layer_sensitivity_values = {}\n",
    "\n",
    "for index, layer_value in enumerate(sensitivities_values):\n",
    "    layer_sensitivity_values[\"layer_\"+str(index)] = layer_value\n",
    "\n",
    "print(layer_sensitivity_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, base_layer, bit_precision):\n",
    "        super(QuantizedLinear, self).__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.bit_precision = bit_precision\n",
    "        self.stored_weight = base_layer.weight.detach().clone()\n",
    "    def calculate_memory_reduction(self):\n",
    "        full_memory = self.stored_weight.nelement() * 32  # Assuming original weights are 32-bit floats\n",
    "        reduced_memory = self.stored_weight.nelement() * self.bit_precision\n",
    "        savings_percentage = 100 * (1 - reduced_memory / full_memory)\n",
    "        memory_ratio = full_memory / reduced_memory\n",
    "        return full_memory, reduced_memory, savings_percentage, memory_ratio\n",
    "\n",
    "    def quantize(self, tensor, bits):\n",
    "        lower_bound = -(2 ** (bits - 1))\n",
    "        upper_bound = (2 ** (bits - 1)) - 1\n",
    "        tensor_min, tensor_max = tensor.min(), tensor.max()\n",
    "        scaling_factor = (tensor_max - tensor_min) / (upper_bound - lower_bound)\n",
    "        scaling_factor = max(scaling_factor, 1e-8)\n",
    "        offset = lower_bound - tensor_min / scaling_factor\n",
    "        quantized_tensor = torch.round(tensor / scaling_factor + offset)\n",
    "        quantized_tensor.clamp_(lower_bound, upper_bound)\n",
    "        quantized_tensor = (quantized_tensor - offset) * scaling_factor\n",
    "        return quantized_tensor\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        processed_weight = self.quantize(self.base_layer.weight, self.bit_precision)\n",
    "        self.base_layer.weight = nn.Parameter(processed_weight)\n",
    "        result = self.base_layer(input_tensor)\n",
    "        self.base_layer.weight = nn.Parameter(self.stored_weight)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model_layers(model, precision_map):\n",
    "    \"\"\"\n",
    "    Quantize model layers based on precision map and calculate memory savings.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to quantize\n",
    "        precision_map: Dictionary mapping layer numbers to bit precision\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (quantized_model, original_memory, quantized_memory, compression_ratio)\n",
    "    \"\"\"\n",
    "    total_original_mem = 0\n",
    "    total_quantized_mem = 0\n",
    "    \n",
    "    # Collect all layers first to avoid OrderedDict mutation issues\n",
    "    all_modules = []\n",
    "    for idx, (name, module) in enumerate(model.named_modules()):\n",
    "        all_modules.append((idx, name, module))\n",
    "    \n",
    "    # Process each module\n",
    "    for _, name, module in all_modules:\n",
    "        # Extract layer number from name using regex\n",
    "        layer_num_match = re.findall(r'\\d+', name)\n",
    "        layer_num = int(layer_num_match[0]) if layer_num_match else 0\n",
    "        \n",
    "        # Get precision for this layer (default to 8-bit if not specified)\n",
    "        bits = precision_map.get(layer_num, 8)\n",
    "        \n",
    "        # Handle different module types\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Quantize linear layer\n",
    "            quant_layer = QuantizedLinear(module, bits)\n",
    "            setattr(model, name, quant_layer)\n",
    "            model._modules[name] = quant_layer\n",
    "            \n",
    "            # Calculate memory usage\n",
    "            orig_mem, quant_mem, _, compression_ratio = quant_layer.calculate_memory_reduction()\n",
    "            total_original_mem += orig_mem\n",
    "            total_quantized_mem += quant_mem\n",
    "            \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            # Quantize layer normalization\n",
    "            quant_layer = QuantizedLinear(module, bits)\n",
    "            setattr(model, name, quant_layer)\n",
    "            model._modules[name] = quant_layer\n",
    "            \n",
    "            # Calculate memory usage\n",
    "            orig_mem, quant_mem, _, compression_ratio = quant_layer.calculate_memory_reduction()\n",
    "            total_original_mem += orig_mem\n",
    "            total_quantized_mem += quant_mem\n",
    "            \n",
    "        elif isinstance(module, nn.MultiheadAttention):\n",
    "            # Quantize attention components\n",
    "            for component_name in ['in_proj_weight', 'in_proj_bias', 'out_proj.weight', 'out_proj.bias']:\n",
    "                param = getattr(module, component_name, None)\n",
    "                if param is not None:\n",
    "                    quant_layer = QuantizedLinear(param, bits)\n",
    "                    setattr(module, component_name, quant_layer.param)\n",
    "                    \n",
    "                    # Calculate memory usage\n",
    "                    orig_mem = param.nelement() * 32  # FP32\n",
    "                    quant_mem = param.nelement() * bits\n",
    "                    total_original_mem += orig_mem\n",
    "                    total_quantized_mem += quant_mem\n",
    "                    \n",
    "    total_orig_mem_mb = total_original_mem / (8 * 1024 * 1024)\n",
    "    total_quant_mem_mb = total_quantized_mem / (8 * 1024 * 1024)\n",
    "\n",
    "    return model, total_orig_mem_mb, total_quant_mem_mb, compression_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 872/872 [00:11<00:00, 77.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9277522935779816"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities = np.array(list(layer_sensitivity_values.values())).reshape(-1, 1)\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(sensitivities)\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 2, 0, 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 8, 1: 8, 2: 8, 3: 12, 4: 12, 5: 8, 6: 8, 7: 4, 8: 8, 9: 4, 10: 4, 11: 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivities = np.array(list(layer_sensitivity_values.values())).reshape(-1, 1)\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(sensitivities)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "cluster_means = {}\n",
    "for cluster_id in range(3):\n",
    "    cluster_indices = np.where(clusters == cluster_id)[0]\n",
    "    cluster_sensitivities = [list(layer_sensitivity_values.values())[i] for i in cluster_indices]\n",
    "    cluster_means[cluster_id] = np.mean(cluster_sensitivities)\n",
    "\n",
    "sorted_clusters = sorted(cluster_means.items(), key=lambda x: x[1], reverse=True)\n",
    "most_sensitive_cluster = sorted_clusters[0][0]\n",
    "medium_sensitive_cluster = sorted_clusters[1][0]\n",
    "least_sensitive_cluster = sorted_clusters[2][0]\n",
    "\n",
    "layer_precision_map = {}\n",
    "for i, (layer_name, sensitivity) in enumerate(layer_sensitivity_values.items()):\n",
    "    if clusters[i] == most_sensitive_cluster:\n",
    "        layer_precision_map[i] = 12  # Highest precision\n",
    "    elif clusters[i] == medium_sensitive_cluster:\n",
    "        layer_precision_map[i] = 8   # Medium precision\n",
    "    else:  # least sensitive cluster\n",
    "        layer_precision_map[i] = 4   # Lowest precision\n",
    "\n",
    "layer_precision_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model, total_orig_memory, total_quant_memory, compression_ratio = quantize_model_layers(model, layer_precision_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original memory: 326.33 MB\n",
      "Quantized memory: 74.83 MB\n",
      "Compression ratio: 4.00x\n"
     ]
    }
   ],
   "source": [
    "# Convert from bits to megabytes (8 bits = 1 byte, 1024*1024 bytes = 1 MB)\n",
    "total_orig_mem_mb = total_orig_memory \n",
    "total_quant_mem_mb = total_quant_memory\n",
    "print(f\"Original memory: {total_orig_mem_mb:.2f} MB\")\n",
    "print(f\"Quantized memory: {total_quant_mem_mb:.2f} MB\")\n",
    "print(f\"Compression ratio: {compression_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 872/872 [00:12<00:00, 70.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9277522935779816"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_accuracy(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory Reduction: Original Memory = 326.3291015625 MB, Quantized Memory = 74.830810546875 MB, Reduction = 77.07%\n",
      "Compression ratio = 4.00\n"
     ]
    }
   ],
   "source": [
    "total_reduction_percent = 100 * (1 - total_quant_mem_mb / total_orig_mem_mb)\n",
    "print(f\"Total Memory Reduction: Original Memory = {total_orig_mem_mb} MB, Quantized Memory = {total_quant_mem_mb} MB, Reduction = {total_reduction_percent:.2f}%\")\n",
    "print(f\"Compression ratio = {compression_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(\"new_quantization\"):\n",
    "    os.mkdir(\"new_quantization\")\n",
    "    \n",
    "torch.save(quantized_model.state_dict(), \"new_quantization/quantized_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"new_quantization/original_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Positive', 0.9999507665634155)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = quantized_model(**inputs)\n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_idx = torch.argmax(scores, dim=-1).item()\n",
    "    confidence = scores[0][pred_idx].item()\n",
    "    label = \"Positive\" if pred_idx == 1 else \"Negative\"\n",
    "\n",
    "    return label, confidence\n",
    "\n",
    "text1  = \"This movie was fantastic! I really enjoyed it.\"\n",
    "predict(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to pi_bert_new_method/\n",
      "- Model weights: pi_bert_new_method/quantized_model.pth\n",
      "- Model config: pi_bert_new_method/model_config/config.json\n",
      "- Tokenizer: pi_bert_new_method/tokenizer/\n",
      "- Layer precision map: pi_bert_new_method/layer_precision_map.json\n"
     ]
    }
   ],
   "source": [
    "# Save the model, tokenizer, and configuration for easy loading\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Create directory structure for the quantized model\n",
    "save_dir = \"pi_bert_new_method\"\n",
    "model_config_dir = os.path.join(save_dir, \"model_config\")\n",
    "tokenizer_dir = os.path.join(save_dir, \"tokenizer\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(model_config_dir, exist_ok=True)\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "\n",
    "# Save the model state dict\n",
    "torch.save(quantized_model.state_dict(), os.path.join(save_dir, \"quantized_model.pth\"))\n",
    "\n",
    "# Save the model configuration\n",
    "model_config = model.config.to_dict()\n",
    "with open(os.path.join(model_config_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "# Save tokenizer files\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "# Save layer precision map for reference\n",
    "with open(os.path.join(save_dir, \"layer_precision_map.json\"), \"w\") as f:\n",
    "    json.dump({str(k): v for k, v in layer_precision_map.items()}, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {save_dir}/\")\n",
    "print(f\"- Model weights: {save_dir}/quantized_model.pth\")\n",
    "print(f\"- Model config: {save_dir}/model_config/config.json\")\n",
    "print(f\"- Tokenizer: {save_dir}/tokenizer/\")\n",
    "print(f\"- Layer precision map: {save_dir}/layer_precision_map.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Layer precision map: {0: 8, 1: 8, 2: 8, 3: 12, 4: 12, 5: 8, 6: 8, 7: 4, 8: 8, 9: 4, 10: 4, 11: 4}\n",
      "\n",
      "Making predictions with the quantized model:\n",
      "Text: This movie was fantastic! I really enjoyed it.\n",
      "Prediction: Positive (confidence: 1.0000)\n",
      "--------------------------------------------------\n",
      "Text: The acting was terrible and the plot made no sense.\n",
      "Prediction: Negative (confidence: 0.9999)\n",
      "--------------------------------------------------\n",
      "Text: It was okay, but I wouldn't watch it again.\n",
      "Prediction: Negative (confidence: 0.9544)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "def load_quantized_model(model_dir=\"output_bert\", quantization_method=True):\n",
    "    \"\"\"\n",
    "    Load a quantized model from the given directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing the saved model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer, layer_precision_map)\n",
    "    \"\"\"\n",
    "    if quantization_method:\n",
    "        model_dir = \"pi_bert_new_method\"\n",
    "    else:\n",
    "        model_dir = \"output_bert\"\n",
    "\n",
    "    # Load model configuration\n",
    "    config_path = os.path.join(model_dir, \"model_config\", \"config.json\")\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(config_path)\n",
    "    except Exception as e:\n",
    "        config = AutoConfig.from_pretrained(model_dir)\n",
    "    \n",
    "    # Create a model with the same architecture as the original\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"output_bert\")\n",
    "    \n",
    "    if quantization_method:\n",
    "    \n",
    "        # Load the quantized state dict\n",
    "        model_path = os.path.join(model_dir, \"quantized_model.pth\")\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        \n",
    "        # Process the state dict to match the model structure\n",
    "        # This handles the case where the saved model has different layer structure\n",
    "        # (e.g., base_layer components) than the loaded model\n",
    "        new_state_dict = OrderedDict()\n",
    "        \n",
    "        for key, value in state_dict.items():\n",
    "            if \".base_layer.\" in key:\n",
    "                # Remove the base_layer part from keys\n",
    "                new_key = key.replace(\".base_layer\", \"\")\n",
    "                new_state_dict[new_key] = value\n",
    "            else:\n",
    "                new_state_dict[key] = value\n",
    "        \n",
    "        # Load the processed state dict into the model\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    # Ensure model is on CPU\n",
    "    model = model.cpu()\n",
    "    \n",
    "    # Load layer precision map if available\n",
    "    layer_precision_map = {}\n",
    "    precision_map_path = os.path.join(model_dir, \"layer_precision_map.json\")\n",
    "    if os.path.exists(precision_map_path):\n",
    "        with open(precision_map_path, \"r\") as f:\n",
    "            layer_precision_map = json.load(f)\n",
    "            # Convert string keys back to integers\n",
    "            layer_precision_map = {int(k): v for k, v in layer_precision_map.items()}\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    except Exception as e:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    \n",
    "    return model, tokenizer, layer_precision_map\n",
    "\n",
    "def predict(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Make a sentiment prediction using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to classify\n",
    "        model: The loaded model\n",
    "        tokenizer: The loaded tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (label, confidence)\n",
    "    \"\"\"\n",
    "    # Ensure model is on CPU\n",
    "    model = model.to('cpu')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    inference_time = (time.time() - start_time) * 1000  # ms\n",
    "    \n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_idx = torch.argmax(scores, dim=-1).item()\n",
    "    confidence = scores[0][pred_idx].item()\n",
    "    label = \"Positive\" if pred_idx == 1 else \"Negative\"\n",
    "    \n",
    "    return label, confidence, inference_time\n",
    "\n",
    "def print_memory_metrics():\n",
    "    # System memory\n",
    "    vm = psutil.virtual_memory()\n",
    "    swap = psutil.swap_memory()\n",
    "    print(f\"System RAM: {vm.total//2**20} MB, Used: {vm.used//2**20} MB ({vm.percent}%)\")\n",
    "    print(f\"Swap used: {swap.used//2**20} MB ({swap.percent}%)\")\n",
    "    # Process memory\n",
    "    p = psutil.Process(os.getpid())\n",
    "    mi = p.memory_info()\n",
    "    print(f\"Process RSS: {mi.rss//2**20} MB, VMS: {mi.vms//2**20} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    try:\n",
    "        model, tokenizer, precision_map = load_quantized_model(quantization_method=True)\n",
    "        print(\"Model loaded successfully!\")\n",
    "        print(f\"Layer precision map: {precision_map}\")\n",
    "        \n",
    "        # Test with some examples\n",
    "        examples = [\n",
    "        \"This movie was fantastic! I really enjoyed it.\",\n",
    "        \"This movie was terrible. I hated every minute of it.\"\n",
    "    ]\n",
    "        \n",
    "        print(\"\\nMaking predictions with the quantized model:\")\n",
    "        for text in examples:\n",
    "            label, confidence, inference_time = predict(text, model, tokenizer)\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Prediction: {label} (confidence: {confidence:.4f})\")\n",
    "            print(f\"Inference time: {inference_time:.2f} ms\")\n",
    "            print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
