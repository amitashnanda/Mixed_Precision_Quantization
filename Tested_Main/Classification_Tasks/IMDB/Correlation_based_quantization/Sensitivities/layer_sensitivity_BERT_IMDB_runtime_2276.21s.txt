# PWCCA sensitivities for BERT-base-uncased on IMDB
# Calibration: TRAIN set (first 5000 examples)
model: textattack/bert-base-uncased-imdb
num_gpus: 4
batch_size: 64
calibration_samples: 5000
time_s: 2276.21
layer_0	0.024479
layer_1	0.021585
layer_2	0.019146
layer_3	0.016482
layer_4	0.014728
layer_5	0.014347
layer_6	0.014593
layer_7	0.015239
layer_8	0.017558
layer_9	0.022948
layer_10	0.029274
layer_11	0.040099
